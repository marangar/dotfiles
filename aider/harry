#!/bin/bash

GEMINI_TOKEN=$HOME/.config/gemini.token
MISTRAL_TOKEN=$HOME/.config/mistral.token
HUGGINGFACE_TOKEN=$HOME/.config/huggingface.token
OPENROUTER_TOKEN=$HOME/.config/openrouter.token
GROQ_TOKEN=$HOME/.config/groq.token
GITHUBCOPILOT_TOKEN=$HOME/.config/githubcopilot.token
[ -e $GEMINI_TOKEN ] && export GEMINI_API_KEY=$(cat $GEMINI_TOKEN)
[ -e $MISTRAL_TOKEN ] && export MISTRAL_API_KEY=$(cat $MISTRAL_TOKEN)
[ -e $HUGGINGFACE_TOKEN ] && export HUGGINGFACE_API_KEY=$(cat $HUGGINGFACE_TOKEN)
[ -e $OPENROUTER_TOKEN ] && export OPENROUTER_API_KEY=$(cat $OPENROUTER_TOKEN)
[ -e $GROQ_TOKEN ] && export GROQ_API_KEY=$(cat $GROQ_TOKEN)
[ -e $GITHUBCOPILOT_TOKEN ] && export OPENAI_API_BASE="https://api.githubcopilot.com" \
                            && export OPENAI_API_KEY=$(cat $GITHUBCOPILOT_TOKEN) \
                            && copilot_models=( $(curl -sf https://api.githubcopilot.com/models \
                                                -H "Authorization: Bearer $OPENAI_API_KEY" \
                                                -H "Content-Type: application/json" \
                                                -H "Copilot-Integration-Id: vscode-chat" \
                                                | jq -r '.data[].id') )

OPTS="--no-auto-commits --dark-mode --watch-files $@"

models=(
    "gemini/gemini-2.5-pro"
    "gemini/gemini-2.5-flash"
    "mistral/mistral-large-latest"
    "mistral/magistral-medium-2506"
    "mistral/devstral-small-2505"
    "openrouter/qwen/qwen3-235b-a22b:free"
    "openrouter/deepseek/deepseek-r1:free"
    "openrouter/z-ai/glm-4.5-air:free"
    "groq/moonshotai/kimi-k2-instruct"
    "groq/openai/gpt-oss-120b"
    "groq/deepseek-r1-distill-llama-70b"
)
if [ ${#copilot_models[@]} -gt 0 ]; then
    models+=( "${copilot_models[@]/#/openai\/}" )
fi
context_size=(
    "1000000"
    "1000000"
    "128000"
    "128000"
    "128000"
    "40960"
    "163840"
    "131072"
    "131072"
    "131072"
    "131072"
)
if [ ${#copilot_models[@]} -gt 0 ]; then
    # add conservative 64K for all copilot models
    context_size+=( $(yes 64000 | head -n "${#copilot_models[@]}") )
fi
if [ -z "$MODEL" ]; then
    echo "Available Models:"
    for i in "${!models[@]}"; do
        echo "[$((i+1))] - ${models[$i]}"
    done
    echo
    read -p "Enter model number (default: 1): " choice
    if [[ -z "$choice" ]]; then
        model_index=0
    elif [[ "$choice" =~ ^[0-9]+$ ]] && (( choice >= 1 && choice <= ${#models[@]} )); then
        model_index=$((choice-1))
    else
        echo "Invalid choice. Exiting ..."
        exit 1
    fi
    MODEL="${models[$model_index]}"
    CTX_SIZE=$(( ${context_size[$model_index]} * 98 / 100 ))
else
    echo "Using model from MODEL environment variable: $MODEL"
    CTX_SIZE=0
fi

if [ -d .git ]; then
    USE_GIT=Y
    readarray -t FILES < <(
        git ls-files |
            while IFS= read -r file; do
                if ! file --mime "$file" | grep -q binary; then
                    echo "$file"
                fi
            done
    )
    REPO_SIZE=$(du --block-size=1 -c "${FILES[@]}" | tail -1 | cut -f1)
    # 1 token ≈ 4 characters, 1 character ≈ 1 byte
    if [ $REPO_SIZE -lt $((CTX_SIZE*4)) ]; then
        OPTS+=" --map-tokens 0"
    else
        FILES=()
    fi
fi
if ls CONVENTIONS.md 2> /dev/null; then
    RFILES="--read CONVENTIONS.md"
fi
[ -z "$USE_GIT" ] && OPTS+=" --no-git"

aider --model "$MODEL" $OPTS "${FILES[@]}" $RFILES
